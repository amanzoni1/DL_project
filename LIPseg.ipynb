{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbPlD5BP+fmvIy3AYndnuM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanzoni1/DL_project/blob/main/LIPseg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Overview\n",
        "\n",
        "In the realm of computer vision, **instance segmentation** is a critical task with applications ranging from autonomous driving to augmented reality. Our project focuses on developing a sophisticated image processing pipeline that can segment people from images and seamlessly replace the background with various cities or tourist spots. This not only enhances visual aesthetics but also has potential applications in photography, virtual backgrounds for video conferencing, and creative media.\n",
        "\n",
        "We will leverage the **LIP (Look Into Person)** dataset, specifically focusing on the **‚Äòperson‚Äô** category. By utilizing advanced segmentation models such as **HRNet**, **DeepLabV3+**, and **U¬≤-Net**, we aim to perform precise segmentation of individuals in images. The project aims to deliver high-quality results suitable for practical use."
      ],
      "metadata": {
        "id": "wzBoNsbhq80e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Objectives\n",
        "\n",
        "### 1. Develop and Train a Neural Network Model:\n",
        "- **Utilize Pre-trained Models:** Leverage pre-trained segmentation models like HRNet, DeepLabV3+, and U¬≤-Net and fine-tune them for our specific task.\n",
        "\n",
        "### 2. Implement Semantic Segmentation:\n",
        "- **Accurate Person Segmentation:** Accurately segment people from images using advanced deep learning techniques.\n",
        "\n",
        "### 3. Background Replacement:\n",
        "- **Seamless Integration:** Replace the original background with selected images of cities or tourist spots while maintaining the integrity of the foreground subject.\n",
        "- **Realistic Blending:** Ensure realistic blending between the foreground and new background to maintain visual aesthetics.\n",
        "\n",
        "### 4. Utilize the LIP Dataset:\n",
        "- **Data Handling:** Work with the LIP dataset containing images and segmentation masks to train and validate our models effectively.\n",
        "\n",
        "### 5. Create an Interactive Pipeline:\n",
        "- **User-Friendly Interface:** Develop a user-friendly interface within Colab for testing the model with custom images and backgrounds in real-time."
      ],
      "metadata": {
        "id": "mhthBZ-JrCBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Details\n",
        "\n",
        "### Dataset\n",
        "\n",
        "#### LIP (Look Into Person) Dataset:\n",
        "- **Description:** A dataset focused on human parsing, containing images of people with detailed segmentation masks for various body parts.\n",
        "- **Usage in Project:** We‚Äôll focus on images containing the ‚Äòperson‚Äô category. The dataset is already split into **Train** and **Val** folders, each containing images, segmentation masks, and corresponding ID files (`train_id.txt` and `val_id.txt`).\n",
        "\n",
        "### Task\n",
        "\n",
        "Develop a pipeline that can:\n",
        "1. **Segment Individuals:** Perform high-accuracy segmentation of people in images.\n",
        "2. **Replace Backgrounds:** Replace the original background with new backgrounds while preserving the foreground subject‚Äôs details.\n",
        "3. **Maintain Realistic Blending:** Ensure that the integration between the foreground and new background appears seamless and natural."
      ],
      "metadata": {
        "id": "jsThGpGMrGOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach\n",
        "\n",
        "1. **Exploratory Data Analysis (EDA):**\n",
        "   - Understand the dataset‚Äôs structure and contents.\n",
        "   - Visualize sample images and annotations to gain insights.\n",
        "\n",
        "2. **Data Preparation:**\n",
        "   - Implement a custom dataset class to load images and annotations.\n",
        "   - Apply data transformations and augmentations to enhance model robustness.\n",
        "\n",
        "3. **Model Setup:**\n",
        "   - Initialize advanced semantic segmentation models (HRNet, DeepLabV3+, U¬≤-Net).\n",
        "   - Modify the models to suit our specific segmentation task.\n",
        "\n",
        "4. **Model Training:**\n",
        "   - Fine-tune the models using the prepared dataset.\n",
        "   - Monitor training progress and optimize performance.\n",
        "\n",
        "5. **Evaluation:**\n",
        "   - Assess the models‚Äô performance using appropriate metrics.\n",
        "   - Visualize predictions to qualitatively evaluate segmentation quality.\n",
        "\n",
        "6. **Background Replacement Pipeline:**\n",
        "   - Develop functions to replace the background of segmented images.\n",
        "   - Ensure seamless integration between the foreground and new background.\n",
        "\n",
        "7. **Interactive Testing:**\n",
        "   - Create an interface in Colab for users to upload images and select backgrounds.\n",
        "   - Allow real-time testing of the segmentation and background replacement."
      ],
      "metadata": {
        "id": "7YkS8sukrKeq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0trJXlIboEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEfEN9pwqKqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "YfjNECBarOKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üöÄ Setup: Install Required Packages\n",
        "# ============================================\n",
        "\n",
        "# Install necessary packages quietly to avoid cluttering the output\n",
        "!pip install -q torchinfo pycocotools albumentations==1.2.1 opencv-python matplotlib\n",
        "\n",
        "# ============================================\n",
        "# üìö Import Standard Libraries\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import statistics\n",
        "import logging\n",
        "\n",
        "# ============================================\n",
        "# üé® Import Visualization Libraries\n",
        "# ============================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# ============================================\n",
        "# üß† Import Deep Learning Libraries\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "\n",
        "# ============================================\n",
        "# üîß Import Additional Libraries\n",
        "# ============================================\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from google.colab import drive, files\n",
        "from IPython.display import display\n",
        "\n",
        "# ============================================\n",
        "# ‚öôÔ∏è Set Computational Device\n",
        "# ============================================\n",
        "\n",
        "# Check if CUDA (GPU) is available and set the device accordingly\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3990O6CAqKk2",
        "outputId": "91727d49-4c89-44e3-e149-23be64275063"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcNqve6dqKaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wqn-dT1aqKXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive and Prepare Dataset\n",
        "\n",
        "Now, I‚Äôm mounting my Google Drive to access the images I‚Äôve already downloaded in the **LIP** dataset directory. I‚Äôll also ensure that both the **Train** and **Val** folders, along with their corresponding segmentation masks and ID files, are properly organized for seamless data loading and processing."
      ],
      "metadata": {
        "id": "dWJyyGrGrcxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset directories\n",
        "train_images_dir = '/content/drive/MyDrive/deep_learning/dataset/LIP/Train/images'\n",
        "train_masks_dir = '/content/drive/MyDrive/deep_learning/dataset/LIP/Train/segmentations'\n",
        "train_ids_file = '/content/drive/MyDrive/deep_learning/dataset/LIP/Train/train_id.txt'\n",
        "\n",
        "val_images_dir = '/content/drive/MyDrive/deep_learning/dataset/LIP/Val/images'\n",
        "val_masks_dir = '/content/drive/MyDrive/deep_learning/dataset/LIP/Val/segmentations'\n",
        "val_ids_file = '/content/drive/MyDrive/deep_learning/dataset/LIP/Val/val_id.txt'\n",
        "\n",
        "# Verify directories\n",
        "os.makedirs(train_images_dir, exist_ok=True)\n",
        "os.makedirs(train_masks_dir, exist_ok=True)\n",
        "os.makedirs(val_images_dir, exist_ok=True)\n",
        "os.makedirs(val_masks_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBG2okTKqKUP",
        "outputId": "e8f0dae5-c935-4acf-9373-8f4335e870a9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l50sGpm0qKRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QO_2VbvSqKPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this section, I will perform an exploratory data analysis (EDA) to gain a deeper understanding of our dataset and ensure it is properly prepared for model training. I will start by displaying some sample images along with their annotations‚Äîbounding boxes and segmentation masks‚Äîto visually confirm that the data is correctly loaded and annotated. Then, I will analyze the distribution of images and annotations by checking how many persons are present in each image, which will help me understand the dataset‚Äôs characteristics and how it aligns with our project‚Äôs goals. Additionally, I will examine the image sizes to assess the variability in dimensions, which may inform any necessary preprocessing steps. By verifying the segmentation masks and ensuring that the images and annotations are properly aligned, I aim to confirm that our dataset is accurately prepared and suitable for proceeding to the model training phase."
      ],
      "metadata": {
        "id": "AUlnB2tVrvqt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0lfbyTTqKMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def display_image_with_annotations(images_dir, masks_dir, img_id):\n",
        "    # Load image\n",
        "    img_path = os.path.join(images_dir, f\"{img_id}.jpg\")\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    # Load mask\n",
        "    mask_path = os.path.join(masks_dir, f\"{img_id}.png\")\n",
        "    mask = Image.open(mask_path).convert(\"L\")\n",
        "    mask_np = np.array(mask)\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 7))\n",
        "    ax.imshow(img_np)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Image ID: {img_id} with Annotations\")\n",
        "\n",
        "    # Overlay mask\n",
        "    ax.imshow(mask_np, cmap='jet', alpha=0.5)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_OV4mS_gqKIe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a random image ID from the training set\n",
        "sample_img_id = random.choice(open(train_ids_file).read().splitlines())\n",
        "\n",
        "# Display the image with annotations\n",
        "display_image_with_annotations(train_images_dir, train_masks_dir, sample_img_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "kvJOOb73qKFg",
        "outputId": "24f009d7-c44a-4025-e602-8f10ea20c6a5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/deep_learning/dataset/LIP/Train/segmentations/133940_532280.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d36612b2a46b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the image with annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdisplay_image_with_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_img_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-4c10b06e396c>\u001b[0m in \u001b[0;36mdisplay_image_with_annotations\u001b[0;34m(images_dir, masks_dir, img_id)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{img_id}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmask_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/deep_learning/dataset/LIP/Train/segmentations/133940_532280.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9Raa9CrqKC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display multiple random samples\n",
        "for _ in range(3):\n",
        "    sample_img_id = random.choice(open(train_ids_file).read().splitlines())\n",
        "    display_image_with_annotations(train_images_dir, train_masks_dir, sample_img_id)"
      ],
      "metadata": {
        "id": "WQdBU3P1qJ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jnpPG-oqJ8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDmoMGqeqJ5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the number of persons per image\n",
        "num_annotations = []\n",
        "\n",
        "with open(train_ids_file, 'r') as f:\n",
        "    train_ids = f.read().splitlines()\n",
        "\n",
        "for img_id in train_ids:\n",
        "    mask_path = os.path.join(train_masks_dir, f\"{img_id}.png\")\n",
        "    mask = Image.open(mask_path).convert(\"L\")\n",
        "    mask_np = np.array(mask)\n",
        "    # Assuming each person is labeled uniquely; adjust if multiple labels per person\n",
        "    num_persons = np.max(mask_np)  # Number of unique labels corresponds to number of persons\n",
        "    num_annotations.append(num_persons)\n",
        "\n",
        "\n",
        "mean_ann = statistics.mean(num_annotations)\n",
        "median_ann = statistics.median(num_annotations)\n",
        "max_ann = max(num_annotations)\n",
        "min_ann = min(num_annotations)\n",
        "\n",
        "print(f\"Average number of persons per image: {mean_ann:.2f}\")\n",
        "print(f\"Median number of persons per image: {median_ann}\")\n",
        "print(f\"Max number of persons in an image: {max_ann}\")\n",
        "print(f\"Min number of persons in an image: {min_ann}\")\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(num_annotations, bins=range(1, max_ann+2), edgecolor='black')\n",
        "plt.title(\"Distribution of Number of Persons per Image\")\n",
        "plt.xlabel(\"Number of Persons\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.xticks(range(1, max_ann + 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uF4YHOXVqJ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gavrTjKRqJz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouQlpPK-qJw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze image sizes\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for img_id in train_ids[:1000]:  # Limiting to first 1000 images for efficiency\n",
        "    img_path = os.path.join(train_images_dir, f\"{img_id}.jpg\")\n",
        "    img = Image.open(img_path)\n",
        "    widths.append(img.width)\n",
        "    heights.append(img.height)\n",
        "\n",
        "# Plot histograms\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].hist(widths, bins=20, edgecolor='black')\n",
        "axes[0].set_title('Distribution of Image Widths')\n",
        "axes[0].set_xlabel('Width (pixels)')\n",
        "axes[0].set_ylabel('Number of Images')\n",
        "\n",
        "axes[1].hist(heights, bins=20, edgecolor='black')\n",
        "axes[1].set_title('Distribution of Image Heights')\n",
        "axes[1].set_xlabel('Height (pixels)')\n",
        "axes[1].set_ylabel('Number of Images')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rsEKXFHmqJuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuUcrIP5qJrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOrMkUUsqJo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_with_masks(images_dir, masks_dir, img_id):\n",
        "    # Load image\n",
        "    img_path = os.path.join(images_dir, f\"{img_id}.jpg\")\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    # Load mask\n",
        "    mask_path = os.path.join(masks_dir, f\"{img_id}.png\")\n",
        "    mask = Image.open(mask_path).convert(\"L\")\n",
        "    mask_np = np.array(mask)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(img_np)\n",
        "    plt.imshow(mask_np, cmap='jet', alpha=0.5)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Image ID: {img_id} with Segmentation Masks\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "D7PZHuOtqJmL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Choose a random image ID from the training set\n",
        "sample_img_id = random.choice(train_ids)\n",
        "\n",
        "# Display the image with masks\n",
        "display_image_with_masks(train_images_dir, train_masks_dir, sample_img_id)"
      ],
      "metadata": {
        "id": "8Tk3cxGZqJjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhQYdpDfqJg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YKqaUrkqJeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing images in the first 1000 images\n",
        "missing_images = []\n",
        "\n",
        "for img_id in train_ids[:1000]:  # Check first 1000 images\n",
        "    img_path = os.path.join(train_images_dir, f\"{img_id}.jpg\")\n",
        "    if not os.path.exists(img_path):\n",
        "        missing_images.append(img_id)\n",
        "\n",
        "print(f\"Number of missing images: {len(missing_images)}\")\n",
        "if missing_images:\n",
        "    print(\"Missing images:\", missing_images)\n",
        "else:\n",
        "    print(\"All images are accessible.\")"
      ],
      "metadata": {
        "id": "AegcxUlFsdKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fEeUuVLsdHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0ZjsaXdsdE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on our EDA:\n",
        "\n",
        "- **Data Integrity:** Images and annotations are properly aligned, and all files are accessible. There are no missing or corrupted files.\n",
        "- **Annotations:** The dataset contains a diverse range of images with varying numbers of persons, mostly featuring 1 to 2 persons per image, which aligns well with our project‚Äôs focus.\n",
        "- **Image Sizes:** There is variability in image dimensions, indicating that we may need to handle resizing or scaling during preprocessing to ensure consistency.\n",
        "- **Segmentation Masks:** The segmentation masks accurately represent the persons in the images, confirming that our data loading and processing pipelines are functioning as expected.\n",
        "- **Visualization:** Sample images and their annotations look correct, providing visual confirmation that the dataset is correctly prepared for model training.\n",
        "\n",
        "These findings give us confidence to proceed to the next steps, knowing that our dataset is suitable for training a robust and accurate model."
      ],
      "metadata": {
        "id": "Fwz02E7dsoUi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FDIMEeHGsc_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-DAhnyesc88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIgk_OWVsc6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "In this section, we will implement a custom dataset class tailored to the **LIP** dataset structure. This class will handle loading images and masks, applying transformations, and preparing the data for training with our chosen segmentation models.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Custom Dataset Class:** Create a `LIPDataset` class inheriting from `torch.utils.data.Dataset`.\n",
        "2. **Data Transformations:** Apply necessary transformations and augmentations to enhance model robustness.\n",
        "3. **DataLoader Creation:** Initialize `DataLoader` instances for both training and validation datasets."
      ],
      "metadata": {
        "id": "WqO5WlF2ssDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIPDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, ids_file, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images_dir (str): Directory with all the images.\n",
        "            masks_dir (str): Directory with all the segmentation masks.\n",
        "            ids_file (str): Path to the txt file with image IDs.\n",
        "            transforms (callable, optional): A function/transform to apply to the images and masks.\n",
        "        \"\"\"\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Read image IDs from the txt file\n",
        "        with open(ids_file, 'r') as f:\n",
        "            self.ids = f.read().splitlines()\n",
        "\n",
        "        self.ids = [id_.strip() for id_ in self.ids if id_.strip()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            # Get image ID\n",
        "            img_id = self.ids[index]\n",
        "\n",
        "            # Load image\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_id}.jpg\")\n",
        "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "            # Load mask\n",
        "            mask_path = os.path.join(self.masks_dir, f\"{img_id}.png\")\n",
        "            mask = np.array(Image.open(mask_path).convert(\"L\"))  # Assuming masks are single-channel\n",
        "\n",
        "            # Convert mask to binary (person vs background)\n",
        "            # Adjust according to LIP's label encoding\n",
        "            mask = np.where(mask > 0, 1, 0).astype(np.uint8)\n",
        "\n",
        "            # Apply transformations\n",
        "            if self.transforms:\n",
        "                augmented = self.transforms(image=image, mask=mask)\n",
        "                image = augmented['image']\n",
        "                mask = augmented['mask']\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image ID {self.ids[index]}: {e}\")\n",
        "            # Return a dummy sample or skip; here we choose to skip by raising an exception\n",
        "            return None, None"
      ],
      "metadata": {
        "id": "sr1rlXGAsc06"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGsrXq9hscya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "walqO-9Ts9Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Transformation Pipeline\n",
        "\n",
        "Using **Albumentations** ensures that the same transformations are applied consistently to both images and their corresponding masks. This is crucial for maintaining the alignment between images and masks during data augmentation."
      ],
      "metadata": {
        "id": "U7DArjX2tphA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train=True):\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "            A.Resize(width=512, height=512),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.1),\n",
        "            A.RandomBrightnessContrast(p=0.2),\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.GaussNoise(p=0.2),\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n",
        "            A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                        std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ], additional_targets={'mask': 'mask'})\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(width=512, height=512),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                        std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ], additional_targets={'mask': 'mask'})"
      ],
      "metadata": {
        "id": "1r983J70s9OD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7xKFLRgs9LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dataset Instances\n",
        "\n",
        "Initialize the `LIPDataset` class for both training and validation datasets, applying the defined transformations."
      ],
      "metadata": {
        "id": "7dJCWDq0t75S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset instances\n",
        "train_dataset = LIPDataset(\n",
        "    images_dir=train_images_dir,\n",
        "    masks_dir=train_masks_dir,\n",
        "    ids_file=train_ids_file,\n",
        "    transforms=get_transform(train=True)\n",
        ")\n",
        "\n",
        "val_dataset = LIPDataset(\n",
        "    images_dir=val_images_dir,\n",
        "    masks_dir=val_masks_dir,\n",
        "    ids_file=val_ids_file,\n",
        "    transforms=get_transform(train=False)\n",
        ")\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "4X9J7H1Bs9Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIAr1JQms9BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataLoaders\n",
        "\n",
        "Initialize `DataLoader` instances for both training and validation datasets. The `collate_fn` handles batches, especially when dealing with potential `None` samples due to errors during data loading."
      ],
      "metadata": {
        "id": "5nIElvS1uEHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Filter out samples where either image or mask is None\n",
        "    batch = [sample for sample in batch if sample[0] is not None and sample[1] is not None]\n",
        "    if not batch:\n",
        "        return None, None\n",
        "    images, masks = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    masks = torch.stack(masks, dim=0)\n",
        "    return images, masks"
      ],
      "metadata": {
        "id": "AbgPd1i-scv9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "ySy8uFW_sctm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKgqEq2kscrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Samples to Verify Transformations\n",
        "\n",
        "Before proceeding to training, it's essential to visualize some samples to ensure that transformations are correctly applied and that masks align perfectly with images."
      ],
      "metadata": {
        "id": "ujt0SU7Zui_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(image, mask):\n",
        "    # Convert image tensor to numpy array\n",
        "    image = image.permute(1, 2, 0).cpu().numpy()\n",
        "    # Reverse normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "\n",
        "    # Get mask\n",
        "    mask = mask.cpu().numpy()\n",
        "\n",
        "    # Plot image and mask\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
        "\n",
        "    ax[0].imshow(image)\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(image)\n",
        "    ax[1].imshow(mask, cmap='jet', alpha=0.5)\n",
        "    ax[1].set_title('Overlayed Mask')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HhHxt0Acscl7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of training data\n",
        "data_iter = iter(train_loader)\n",
        "images, masks = next(data_iter)\n",
        "\n",
        "# Visualize the first sample in the batch\n",
        "visualize_sample(images[0], masks[0])"
      ],
      "metadata": {
        "id": "5NjLiSryqJbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wV0oBLwtuGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBwrCO5luGG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ReslYeJ9uGET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup\n",
        "\n",
        "In this section, we will define and initialize our semantic segmentation models: **HRNet**, **DeepLabV3+**, and **U¬≤-Net**. We will focus on **DeepLabV3+** for this implementation example. The same principles apply to the other models, with necessary adjustments based on their architectures."
      ],
      "metadata": {
        "id": "70IYbvHJurV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DeepLabV3+ Model\n",
        "def get_deeplabv3_plus(num_classes):\n",
        "    model = deeplabv3_resnet50(pretrained=True, progress=True)\n",
        "    # Replace the classifier\n",
        "    model.classifier = DeepLabHead(2048, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "AFqMwCv5urGc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of classes (background + person)\n",
        "num_classes = 2\n",
        "\n",
        "# Initialize the model\n",
        "model = get_deeplabv3_plus(num_classes)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "QCx4nOvuuGBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pu2J_KCCuF-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "summary(model, input_size=(1, 3, 512, 512))"
      ],
      "metadata": {
        "id": "sl3SBY86uF8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWWI37yTuF5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vzqab-yCvDeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer and Learning Rate Scheduler\n",
        "\n",
        "Configure the optimizer and learning rate scheduler to train the model effectively."
      ],
      "metadata": {
        "id": "jyRlRHfXvASQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer (Adam is commonly used for DeepLabV3+)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "PGUmT4CDuF3G",
        "outputId": "fab3fe4d-79c8-4c4d-a31d-b8931dca3ff6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-12abf928380c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define optimizer (Adam is commonly used for DeepLabV3+)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define learning rate scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbOcAxDhu-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjChgkgYu-RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsDRqZe7u-Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop with Error Handling\n",
        "\n",
        "Implement a robust training loop that includes error handling to prevent the training process from stopping due to unexpected errors. Errors will be logged for later analysis."
      ],
      "metadata": {
        "id": "Ni-NMoO-vL6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='semantic_training_errors.log',  # Log file name\n",
        "    filemode='a',                             # Append mode\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    level=logging.ERROR                       # Log only errors\n",
        ")\n",
        "\n",
        "# Initialize a list to keep track of failed batches (optional)\n",
        "failed_batches = []\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batches_completed = 0\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        try:\n",
        "            # Move images and masks to device\n",
        "            images = images.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            masks = masks.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)['out']\n",
        "            loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, masks.float())\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            epoch_loss += loss.item()\n",
        "            batches_completed += 1\n",
        "\n",
        "            # Update tqdm with loss information\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the error with epoch and batch information\n",
        "            logging.error(f\"Epoch {epoch+1}, Batch {batch_idx+1}: {str(e)}\")\n",
        "\n",
        "            # Optionally, keep track of failed batches\n",
        "            failed_batches.append((epoch+1, batch_idx+1))\n",
        "\n",
        "            # Continue to the next batch\n",
        "            continue\n",
        "\n",
        "    # Adjust learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Print epoch loss\n",
        "    if batches_completed > 0:\n",
        "        average_loss = epoch_loss / batches_completed\n",
        "    else:\n",
        "        average_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Save model checkpoint\n",
        "    torch.save(model.state_dict(), f'deeplabv3plus_lip_segmentation_epoch_{epoch+1}.pth')\n",
        "\n",
        "    # Optionally, evaluate on validation set\n",
        "    # Implement evaluation logic here"
      ],
      "metadata": {
        "id": "my5kIEsLu-L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "budsyRDCu-JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XR_BNlxu-Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UguX6D2bu-D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "After training, we need to assess the model‚Äôs performance using appropriate metrics. For semantic segmentation, metrics like **Intersection over Union (IoU)** and **Pixel Accuracy** are commonly used.\n",
        "\n",
        "### Evaluation Steps:\n",
        "1. **Compute IoU:** Measure the overlap between the predicted masks and ground truth masks.\n",
        "2. **Compute Pixel Accuracy:** Calculate the proportion of correctly classified pixels.\n",
        "3. **Visualize Predictions:** Qualitatively evaluate the segmentation quality by visualizing model predictions."
      ],
      "metadata": {
        "id": "bwH4fAfqvgNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_semantic(model, data_loader, device):\n",
        "    model.eval()\n",
        "    iou_scores = []\n",
        "    pixel_accuracies = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            preds = torch.sigmoid(outputs) > 0.5  # Binary masks\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                # Compute IoU\n",
        "                intersection = (pred & mask.bool()).sum().item()\n",
        "                union = (pred | mask.bool()).sum().item()\n",
        "                iou = intersection / union if union != 0 else 0\n",
        "                iou_scores.append(iou)\n",
        "\n",
        "                # Compute Pixel Accuracy\n",
        "                correct = (pred == mask.bool()).sum().item()\n",
        "                total = mask.numel()\n",
        "                pixel_acc = correct / total\n",
        "                pixel_accuracies.append(pixel_acc)\n",
        "\n",
        "    average_iou = np.mean(iou_scores)\n",
        "    average_pixel_acc = np.mean(pixel_accuracies)\n",
        "\n",
        "    print(f\"Average IoU: {average_iou:.4f}\")\n",
        "    print(f\"Average Pixel Accuracy: {average_pixel_acc:.4f}\")"
      ],
      "metadata": {
        "id": "TPsS41t6vf6N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform evaluation on the validation set\n",
        "evaluate_model_semantic(model, val_loader, 'cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "wmDPKKr3u-Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEaOGs8xu9-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdQX_hNGu98H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Model Predictions\n",
        "\n",
        "To qualitatively assess the segmentation quality, visualize some model predictions alongside the original images and ground truth masks."
      ],
      "metadata": {
        "id": "usciDKIVvpx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions_semantic(model, dataset, device, num_samples=5):\n",
        "    model.eval()\n",
        "    for i in range(num_samples):\n",
        "        img, mask = dataset[i]\n",
        "        with torch.no_grad():\n",
        "            input_img = img.unsqueeze(0).to(device)\n",
        "            output = model(input_img)['out']\n",
        "            pred_mask = torch.sigmoid(output).squeeze().cpu().numpy()\n",
        "            pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        # Convert image tensor to numpy array\n",
        "        image = img.permute(1, 2, 0).cpu().numpy()\n",
        "        # Reverse normalization\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "        # Ground truth mask\n",
        "        gt_mask = mask.cpu().numpy()\n",
        "\n",
        "        # Plot images and masks\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
        "\n",
        "        ax[0].imshow(image)\n",
        "        ax[0].set_title('Original Image')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        ax[1].imshow(image)\n",
        "        ax[1].imshow(gt_mask, cmap='jet', alpha=0.5)\n",
        "        ax[1].set_title('Ground Truth Mask')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        ax[2].imshow(image)\n",
        "        ax[2].imshow(pred_mask, cmap='jet', alpha=0.5)\n",
        "        ax[2].set_title('Predicted Mask')\n",
        "        ax[2].axis('off')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "zuTD_pptvpmd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize predictions on the validation set\n",
        "visualize_predictions_semantic(model, val_dataset, 'cuda' if torch.cuda.is_available() else 'cpu', num_samples=3)"
      ],
      "metadata": {
        "id": "3T0JsFePu95v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLqgeckku93J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cv722vlvu90y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background Replacement Pipeline\n",
        "\n",
        "With the segmentation model trained and evaluated, the next step is to develop a pipeline that replaces the background of segmented images with new backgrounds. This involves:\n",
        "\n",
        "1. **Extracting the Foreground:** Use the predicted masks to isolate the person from the original image.\n",
        "2. **Selecting a New Background:** Choose an image of a city or tourist spot to serve as the new background.\n",
        "3. **Blending:** Seamlessly blend the foreground with the new background to maintain visual integrity.\n",
        "\n",
        "### Key Functions:\n",
        "- **Mask Application:** Apply the predicted mask to extract the foreground.\n",
        "- **Background Selection:** Load and preprocess the new background image.\n",
        "- **Image Blending:** Combine the foreground and new background with appropriate masking and blending techniques."
      ],
      "metadata": {
        "id": "VxI6P66GvzsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_background(original_image, mask, new_background):\n",
        "    \"\"\"\n",
        "    Replace the background of the original_image with new_background using the provided mask.\n",
        "\n",
        "    Args:\n",
        "        original_image (PIL.Image or np.array): The original image.\n",
        "        mask (np.array): Binary mask where 1 represents the foreground.\n",
        "        new_background (PIL.Image or np.array): The new background image.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Image with the background replaced.\n",
        "    \"\"\"\n",
        "    if isinstance(original_image, Image.Image):\n",
        "        original_image = np.array(original_image)\n",
        "    if isinstance(new_background, Image.Image):\n",
        "        new_background = np.array(new_background)\n",
        "\n",
        "    # Resize new background to match original image\n",
        "    new_background = Image.fromarray(new_background).resize((original_image.shape[1], original_image.shape[0]))\n",
        "    new_background = np.array(new_background)\n",
        "\n",
        "    # Ensure mask is binary\n",
        "    mask = (mask > 0).astype(np.uint8)\n",
        "    mask = np.stack([mask]*3, axis=-1)  # Make it 3-channel\n",
        "\n",
        "    # Blend images\n",
        "    blended = original_image * mask + new_background * (1 - mask)\n",
        "    blended = blended.astype(np.uint8)\n",
        "\n",
        "    return blended\n",
        "\n",
        "# Example usage\n",
        "def example_background_replacement():\n",
        "    # Load original image\n",
        "    img_id = random.choice(train_ids)\n",
        "    img_path = os.path.join(train_images_dir, f\"{img_id}.jpg\")\n",
        "    original_img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Load predicted mask\n",
        "    mask_path = os.path.join(train_masks_dir, f\"{img_id}.png\")\n",
        "    mask = Image.open(mask_path).convert(\"L\")\n",
        "    mask_np = np.array(mask)\n",
        "    mask_np = np.where(mask_np > 0, 1, 0).astype(np.uint8)\n",
        "\n",
        "    # Load new background\n",
        "    background_path = '/content/drive/MyDrive/deep_learning/backgrounds/new_background.jpg'  # Replace with your background path\n",
        "    new_background = Image.open(background_path).convert(\"RGB\")\n",
        "\n",
        "    # Replace background\n",
        "    blended_image = replace_background(original_img, mask_np, new_background)\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_img)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask_np, cmap='gray')\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(blended_image)\n",
        "    plt.title('Background Replaced Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5iPiVnNgu9yW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the example\n",
        "example_background_replacement()"
      ],
      "metadata": {
        "id": "2YhmoF-9u9v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHCDXhd-u9ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kHikC5YVv8sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmVtweDzv8po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Testing\n",
        "\n",
        "To facilitate real-time testing of our segmentation and background replacement pipeline, we will create an interactive interface within Colab. This interface will allow users to upload their own images and select backgrounds for replacement.\n",
        "\n",
        "### Key Features:\n",
        "1. **Image Upload:** Users can upload custom images for segmentation.\n",
        "2. **Background Selection:** Users can choose from a set of predefined backgrounds or upload their own.\n",
        "3. **Real-Time Processing:** Upon selection, the model will segment the person and replace the background, displaying the result instantly."
      ],
      "metadata": {
        "id": "OIzy4H2tv_OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_background_replacement(model, device):\n",
        "    # Upload original image\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        original_image = Image.open(filename).convert(\"RGB\")\n",
        "        display(original_image)\n",
        "\n",
        "        # Preprocess image\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((512, 512)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                 std=(0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "        input_tensor = transform(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)['out']\n",
        "            pred_mask = torch.sigmoid(output) > 0.5\n",
        "            pred_mask = pred_mask.squeeze().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Upload new background\n",
        "        print(\"Upload a new background image:\")\n",
        "        bg_uploaded = files.upload()\n",
        "        for bg_filename in bg_uploaded.keys():\n",
        "            new_background = Image.open(bg_filename).convert(\"RGB\")\n",
        "            display(new_background)\n",
        "\n",
        "        # Replace background\n",
        "        blended_image = replace_background(original_image, pred_mask, new_background)\n",
        "\n",
        "        # Display result\n",
        "        blended_pil = Image.fromarray(blended_image)\n",
        "        display(blended_pil)"
      ],
      "metadata": {
        "id": "NH4Vnkfpv_CX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "interactive_background_replacement(model, 'cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "gnsKLGdAv8nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6_dcnxav8kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHEDbIC-v8h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKUuocL_v8fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this project, we developed a comprehensive image processing pipeline for **person segmentation** using the **LIP (Look Into Person)** dataset. By leveraging advanced semantic segmentation models such as **HRNet**, **DeepLabV3+**, and **U¬≤-Net**, we achieved high-accuracy segmentation of individuals in images. The pipeline not only segments the person but also seamlessly replaces the background with various cityscapes and tourist spots, maintaining realistic blending to ensure visual integrity.\n",
        "\n",
        "### Summary of Achievements:\n",
        "- **Robust Data Handling:** Implemented a custom dataset class tailored to the LIP dataset, ensuring efficient data loading and preprocessing.\n",
        "- **Advanced Model Training:** Trained state-of-the-art segmentation models, fine-tuning them to achieve optimal performance on our specific task.\n",
        "- **Seamless Background Replacement:** Developed a pipeline that accurately replaces backgrounds while preserving the foreground subject's details.\n",
        "- **Interactive Interface:** Created an interactive Colab interface allowing users to test the model with custom images and backgrounds in real-time.\n",
        "- **Comprehensive Evaluation:** Assessed model performance using quantitative metrics and qualitative visualizations to ensure high-quality segmentation results.\n",
        "\n",
        "### Future Work:\n",
        "- **Model Optimization:** Explore further optimizations and fine-tuning techniques to enhance segmentation accuracy.\n",
        "- **Enhanced Augmentations:** Incorporate more diverse data augmentations to improve model generalization.\n",
        "- **Extended Applications:** Apply the pipeline to video data for real-time background replacement in video conferencing applications.\n",
        "- **User Interface Improvements:** Develop a more sophisticated user interface with additional customization options for background selection and blending parameters.\n",
        "\n",
        "This project demonstrates the effectiveness of modern deep learning techniques in achieving precise and practical image segmentation tasks. The developed pipeline holds significant potential for various applications in media, photography, and augmented reality."
      ],
      "metadata": {
        "id": "3Ebn3gP8wSUL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9oBqdfav8cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO37xWEmv8aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElwirnzBv8XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEx9NRuev8U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMf3l4B4v8Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UD65aqiyv8P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yCgZbzRv8Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPC4mIZTv8LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGORO-kluFx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6JulNT4VboBv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdZWiuXXbn_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mzPH0XnAbn80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsdD7WfTbn6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3W3mVT-Abn3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VrUcUzqxUg-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjSVbcmNfBPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mopgyw30fBM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6x8llCefBE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQegd96QfBCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8WBqMsnfA_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IAXve0nlfA85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xy0rSTVTfA6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVhIYkGEfA3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvv5sxaXfA1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2t4KnLmfAys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ds8btZTjfAwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "il5b9KoZfArX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wb51a7gLfAos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qS051d2ftub3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ihYDK9vcUgyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-1bUBSpUgqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGux0cy5Ugn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYfQC3NiUgi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iH5TY4P2UgZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CX2I34zUgWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6my9Gz4esQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9nFUxmiesOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfz912dwesLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "wg5o--0vgaFE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65StXaDegcVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6F42TYJgcTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Parameters of the model that require gradients\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# Define optimizer (Adam is commonly used for DeepLabV3+)\n",
        "optimizer = optim.Adam(params, lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "u25sOazZgcRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1OgCviPgcOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ADVANCED FUNCION LOSS"
      ],
      "metadata": {
        "id": "-REsd9AZesG0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def dice_loss(pred, target, epsilon=1e-6):\n",
        "    pred = torch.sigmoid(pred)\n",
        "    intersection = (pred * target).sum(dim=(2, 3))\n",
        "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
        "    return 1 - dice.mean()"
      ],
      "metadata": {
        "id": "WLJsS5HjmVyg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_loss(pred, target):\n",
        "    ce_loss = F.binary_cross_entropy(pred, target)\n",
        "    dice_loss = 1 - dice_coefficient(pred, target)\n",
        "    return ce_loss + dice_loss\n",
        "\n",
        "def dice_coefficient(pred, target, epsilon=1e-6):\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum()\n",
        "    return (2. * intersection + epsilon) / (union + epsilon)"
      ],
      "metadata": {
        "id": "cHIfxq7Dfswy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "7YJjMea_ferv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syv1slUDfssF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLp0-yQgfspk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "    # Optionally, evaluate on validation set\n",
        "    # Implement evaluation logic here"
      ],
      "metadata": {
        "id": "N3GNrz4UfsnC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "En2Ana4rfskZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USARE MIXED PRECISION TRAINING DA CAPIRE DOVE\n",
        "\n",
        "\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Initialize GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batches_completed = 0\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        try:\n",
        "            # Move images and masks to device\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)['out']\n",
        "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, masks.float())\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batches_completed += 1\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Epoch {epoch+1}, Batch {batch_idx+1}: {str(e)}\")\n",
        "            failed_batches.append((epoch+1, batch_idx+1))\n",
        "            continue\n",
        "\n",
        "    # Adjust learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Print epoch loss\n",
        "    if batches_completed > 0:\n",
        "        average_loss = epoch_loss / batches_completed\n",
        "    else:\n",
        "        average_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Save model checkpoint\n",
        "    torch.save(model.state_dict(), f'deeplabv3plus_lip_segmentation_epoch_{epoch+1}.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "4TCle0B-esEU",
        "outputId": "d107d1da-4919-4079-c96b-d37424241f90"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-5e603f1fb0f1>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5e603f1fb0f1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7jG8MeKesBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKONHPDRer_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQUdDp48f85i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions_semantic(model, dataset, device, num_samples=5):\n",
        "    model.eval()\n",
        "    for i in range(num_samples):\n",
        "        img, mask = dataset[i]\n",
        "        with torch.no_grad():\n",
        "            input_img = img.unsqueeze(0).to(device)\n",
        "            output = model(input_img)['out']\n",
        "            pred_mask = torch.sigmoid(output).squeeze().cpu().numpy()\n",
        "            pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        # Convert image tensor to numpy array\n",
        "        image = img.permute(1, 2, 0).cpu().numpy()\n",
        "        # Reverse normalization\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "        # Ground truth mask\n",
        "        gt_mask = mask.cpu().numpy()\n",
        "\n",
        "        # Plot images and masks\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
        "\n",
        "        ax[0].imshow(image)\n",
        "        ax[0].set_title('Original Image')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        ax[1].imshow(image)\n",
        "        ax[1].imshow(gt_mask, cmap='jet', alpha=0.5)\n",
        "        ax[1].set_title('Ground Truth Mask')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        ax[2].imshow(image)\n",
        "        ax[2].imshow(pred_mask, cmap='jet', alpha=0.5)\n",
        "        ax[2].set_title('Predicted Mask')\n",
        "        ax[2].axis('off')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "flS7Euhmf83I"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "visualize_predictions_semantic(model, val_dataset, device, num_samples=3)"
      ],
      "metadata": {
        "id": "nsklr_Qwg6vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vKeanpFJg6sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVdN7ma4g6qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tocgvbt7g6n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YX7XZfHqg6lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post processing"
      ],
      "metadata": {
        "id": "cXngA9CWf959"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JgeLlOUf80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
        "\n",
        "def refine_mask_crf(image, mask):\n",
        "    height, width = mask.shape\n",
        "    n_labels = 2\n",
        "\n",
        "    # Convert mask to softmax probabilities\n",
        "    mask = mask.astype(np.float32)\n",
        "    mask = np.expand_dims(mask, axis=0)\n",
        "    mask = np.concatenate([1 - mask, mask], axis=0)\n",
        "    unary = unary_from_softmax(mask)\n",
        "    unary = np.ascontiguousarray(unary)\n",
        "\n",
        "    # Initialize CRF\n",
        "    d = dcrf.DenseCRF2D(width, height, n_labels)\n",
        "    d.setUnaryEnergy(unary)\n",
        "\n",
        "    # Add pairwise potentials\n",
        "    d.addPairwiseGaussian(sxy=3, compat=3)\n",
        "    d.addPairwiseBilateral(sxy=80, srgb=13, rgbim=image, compat=10)\n",
        "\n",
        "    # Perform inference\n",
        "    Q = d.inference(5)\n",
        "    refined_mask = np.argmax(Q, axis=0).reshape((height, width))\n",
        "\n",
        "    return refined_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "lJCspkCjf8yN",
        "outputId": "76856425-244d-4457-adc8-3ac93057f91c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pydensecrf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c4fa1c289b7d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydensecrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensecrf\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdcrf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydensecrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munary_from_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_pairwise_bilateral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrefine_mask_crf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydensecrf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def visualize_refined_mask(image, mask):\n",
        "    # Convert image from PIL to NumPy and BGR\n",
        "    image_np = np.array(image)\n",
        "    image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Refine mask with CRF\n",
        "    refined_mask = refine_mask_crf(image_cv, mask)\n",
        "\n",
        "    # Plot original image, initial mask, and refined mask\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
        "\n",
        "    ax[0].imshow(image_np)\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(mask, cmap='gray')\n",
        "    ax[1].set_title('Initial Mask')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    ax[2].imshow(image_np)\n",
        "    ax[2].imshow(refined_mask, cmap='jet', alpha=0.5)\n",
        "    ax[2].set_title('Refined Mask with CRF')\n",
        "    ax[2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage after loading a sample\n",
        "visualize_refined_mask(images[0], masks[0].cpu().numpy())"
      ],
      "metadata": {
        "id": "3rlNoE8of8vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGp9-4fEf8tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dopgZy9yf8qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkvpR8e2f8oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tn3Tu_CHgI7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJyrXwTxgI4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTMAmGJvgI2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXNGs9TPgIz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjSpemAtgIxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkqHmEF8gIu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vvHr1c1DgIsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}